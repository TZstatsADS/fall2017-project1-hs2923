---
title: "ADS - Project 1"
author: "Henrique Saboya Lopes Tavares de Melo (hs2923)"
date: "September 10, 2017"
output: html_document
---

The idea behind this project/document is to argue on two main questions: whether we are able to identify different word patterns on republican speeches when compared to democratic speeches, and, if they show different word patterns, are these patterns consistent within each party?

For that, we'll analyze the 58 inaugural speeches from most US Presidents so far, using text mining, clustering and topic modeling tools.

Thus, we will break down our project into XXX steps:

1) Text Mining: We'll use "tm" package to create a corpus of documents, remove noisy data and create a Document Term Matrix;

2) Clustering: We'll cluster our documents according to the word frequencies used on the speeches;

3) Topic Modeling: We'll use topic modeling tools to verify if Political Parties have significant distinctions in topics;


## Step 0

### Tools

Before we start, let's first load all necessary packages:

```{r, warning=F, message=F}
set.seed(0)

packages.used=c("tm", "wordcloud", "fpc", 
                "ggplot2", "RCurl", "xlsx", "MASS")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE,
                   repos='http://cran.us.r-project.org')
}

library(tm)
library(wordcloud)
library(fpc)
library(ggplot2)
library(RCurl)
library(xlsx)
library(MASS)
```

## Step 1

### Text Mining

Now, we need to import the information that we have been provided with. We have 2 documents, one with speech dates, and the other with detailed information about the presidents, including their political parties which is crucial to our analyses.

```{r}
#Reading file with speech dates from local directory

dir1 <- "D:/Google Drive/Google Drive/My Files/2) Estudos/4) Columbia/3) Classes/4) Fall 2017/1) Applied Data Science/1) GitHub/fall2017-project1-hs2923/data/InauguationDates.txt"

dates.speeches <- read.table(dir1, header = T, skip = 1, sep = "\t")

#Reading Information file from local directory

dir2 <- "D:/Google Drive/Google Drive/My Files/2) Estudos/4) Columbia/3) Classes/4) Fall 2017/1) Applied Data Science/1) GitHub/fall2017-project1-hs2923/data/InaugurationInfo.xlsx"
  
InaugurationInfo <- read.xlsx(dir2, sheetIndex = 1)

rm(dir1,dir2)
```

The next step is to create a corpus with all documents (speeches in our case), so that we can perform our initial text analyzes.

```{r setup, include=FALSE}
#Importing Speech data

dir3 <- "D:/Google Drive/Google Drive/My Files/2) Estudos/4) Columbia/3) Classes/4) Fall 2017/1) Applied Data Science/1) GitHub/fall2017-project1-hs2923/data/InauguralSpeeches"

doc <- DirSource(directory = dir3, encoding = "UTF-8", pattern = "\\.txt$"); rm(dir3)

ovid <- VCorpus(doc, readerControl = list(language = "en"))
```

Let's get a sense of our corpus:

```{r}
ovid

inspect(ovid[1])
```

On the following step, we'll make use of "tm" functions to: standardize all words to lower case, remove stop words, remove punctuations and clean white spaces. 

```{r}
ovid.t <- ovid   #Renaming corpus
ovid.t <- tm_map(ovid.t, content_transformer(tolower))
ovid.t <- tm_map(ovid.t, removeWords, stopwords("english"))
ovid.t <- tm_map(ovid.t, removePunctuation, preserve_intra_word_dashes = T)
ovid.t <- tm_map(ovid.t, stripWhitespace)
```

ovid.t becomes our new transformed corpus.

With our speeches data now cleaned up and organized in a corpus, it is easy to make some analyzes. Let's create a Document Term Matrix to first explore and understand which words are more commonly used by American Presidents. This will later allow us to make deeper analyzes.

```{r}
dtm <- DocumentTermMatrix(ovid.t) #Function to create matrix
dim(dtm)
```

Our Document term Matrix has 58 rows (documents) and 9407 columns (vocabulary words). A brief summary is shown below:

```{r}
inspect(dtm[1:5, 10:13])
```

As expected, we initially have a high level of sparsity (columns with many zeros), however, for now, let's take a look at the overall most common words used by presidents. Using a "rule of thumb", we'll select only the words that appear more than 150 times.

```{r}
findFreqTerms(dtm, 150)
```

It's now clear to see the most common words among President's speeches. First, words such as 'America', 'Country' and 'Citizens' could have been foreseen in any presidential speech, however let's focus on some specific words that may relate to American values. The words 'Free', 'Freedom', 'Peace', 'Power', 'Union'and 'War' have deep ideological meanings which we can relate to important Ideological Values for the United States. 

Making a Word Cloud will help us visualize this information.

```{r, warning=F}
wordcloud(tm_map(ovid.t, PlainTextDocument), min.freq=100, scale=c(5,2),
          random.color=T, max.word=60, random.order=F,colors=brewer.pal(8,"Dark2"))
```

Now, as described before, our Document Term Matrix has more than 9000 words. To make more meaningful analyzes, let's get rid of sparse terms, focusing on the most repeated ones. We are not looking for an optimal sparsity value to filter our data, so let's set a 50% column sparsity to be our cutting boundary.

```{r}
dtm <- removeSparseTerms(dtm, 0.5)
dim(dtm)
inspect(dtm[1:5, 10:13])
dtm.df <- as.data.frame(as.matrix(dtm))
```

We now have a matrix with only 150 terms!

Also, let's merge the information from president speeches to our Document Term matrix.

```{r}
InaugurationInfo$Key <- paste("inaug", InaugurationInfo$File, "-",InaugurationInfo$Term, ".txt", sep = "")
dtm.df$Key <- rownames(dtm.df) #Creates a key which will be used to link the tables

dtm.df <- merge(x = dtm.df, y = InaugurationInfo[,c(4,6)])

#Reshape and adequate the dataframe format before merging
ds <- reshape(dates.speeches, direction = "long", varying = list(2:5), v.names = "SpeechDates") 
ds <- ds[!ds$SpeechDates == "", c(1:3)]
ds$SpeechDates <- as.Date(ds$SpeechDates, "%m/%d/%Y")
names(ds) <- c("President", "Term", "SpeechDate")
ds$Key <- paste("inaug", sapply(ds$President, gsub, pattern = " ", replacement = ""), "-",ds$Term, ".txt", sep = "")

```

## Step 2

### Clustering

Before we proceed with Clustering, let's take a peek at the World Cloud for both Republicans and Democrats, to check if we can already identify some differences.

```{r, warning=F, message=F}
par(mfrow = c(1,2))

wordcloud(words = names(dtm.df[which(dtm.df$Party == "Democratic"), which(sapply(dtm.df, is.numeric))]), freq = colSums(dtm.df[which(dtm.df$Party == "Democratic"), which(sapply(dtm.df, is.numeric))]), min.freq=60, scale=c(5,1), random.color=T, max.word=60, random.order=F,colors=brewer.pal(8,"Dark2"))

text(labels = "Democrats", x = 0.5, y = 1.1, cex = 2)

wordcloud(words = names(dtm.df[which(dtm.df$Party == "Republican"), which(sapply(dtm.df, is.numeric))]), freq = colSums(dtm.df[which(dtm.df$Party == "Republican"), which(sapply(dtm.df, is.numeric))]), min.freq=60, scale=c(5,1), random.color=T, max.word=60, random.order=F,colors=brewer.pal(8,"Dark2"))

text(labels = "Republicans", x = 0.5, y = 1.1, cex = 2)

``` 

Let us also set other parties with the same label.

```{r}
dtm.df$Party <- ifelse(dtm.df$Party == "Republican" | dtm.df$Party == "Democratic", as.character(dtm.df$Party), "Other")
```

At our first look, it is hard to make any conclusions. The words just tend to repeat themselves for both groups.

Nonetheless, with our Document Term Matrix, we are now able to perform some Clustering analyzes. The idea is to group speeches according to the words they tend to be used. We don't need to find an optimal number os clusters, we just need to identify whether we can distinguish Democrats from Republicans by their vocabulary, hence, we'll just try different number of clusters using K-means.

```{r}
Cluster.model2 <- kmeans(dtm, 2, iter.max = 15, nstart = 20)
Cluster.model3 <- kmeans(dtm, 3, iter.max = 15, nstart = 20)
Cluster.model4 <- kmeans(dtm, 4, iter.max = 15, nstart = 20)

dtm.df$Cluster2 <- Cluster.model2$cluster
dtm.df$Cluster3 <- Cluster.model3$cluster
dtm.df$Cluster4 <- Cluster.model4$cluster
```

Clustering our speeches into 2, 3 and 4 clusters, let's understand how they differ.

```{r}
table(dtm.df[ ,c("Party", "Cluster2")])
```

If we look at 2 clusters only, we could assume Republicans and Democrats do seem to have similar vocabularies.

```{r}
table(dtm.df[ ,c("Party", "Cluster3")])
```

Using 3 clusers, we are now able to raise some questions. Most Democrats fall within Cluster 3, however republicans tend to go to Clusters 1 and 2.

```{r}
table(dtm.df[ ,c("Party", "Cluster4")])
```

Breaking down into 4 Clusters, we can see that more than 54% of democrats fall in Cluster 3, and 79% of Republicans fall in Clusters 1 and 2.

To better visualize these Clusters, let's perform a PCA analyzes in order to reduce our problem to 2 dimensions.

```{r}
PCA.model <- prcomp(dtm)

plot((PCA.model$sdev^2)/sum(PCA.model$sdev^2)*100, type = "l", col = "blue", ylab = "Prop. of Variance", xlab = "PC")
points((PCA.model$sdev^2)/sum(PCA.model$sdev^2)*100, pch = 22, bg = "red")

```

Our first 2 Principal Components are capable of explaining approximately 50% of our Variance.

Thus, Let's first look where do our speeches fall according to their Political parties, and then according to their clusters:

```{r}
ggplot(data = as.data.frame(PCA.model$x[,1:2]), aes(PC1, PC2)) +
  geom_point(aes(col = as.factor(dtm.df$Party)))

#2 Clusters

ggplot(data = as.data.frame(PCA.model$x[,1:2]), aes(PC1, PC2)) +
  geom_point(aes(col = as.factor(dtm.df$Cluster2)))

#3 Clusters

ggplot(data = as.data.frame(PCA.model$x[,1:2]), aes(PC1, PC2)) +
  geom_point(aes(col = as.factor(dtm.df$Cluster3)))

#4 Clusters

ggplot(data = as.data.frame(PCA.model$x[,1:2]), aes(PC1, PC2)) +
  geom_point(aes(col = as.factor(dtm.df$Cluster4)))
```

Now that we have seen our contigency tables and had a visual input of our data, what can we conclude so far:

1. There are some patterns on Democratic speeches indeed that are different from Republican speeches, with some exceptions apparently;

2. Most democratic speeches seem to be consistent with each other, however republican speeches tend to diverge from their selves and from other parties, also with exceptions;

The choice of K-Means with PCA was good to have a sense of the Words used and differences among speeches, however when we look at the data, using a non-linear model may seem a good idea if we are to better segregate Democrats from Republicans.

## Step 3

### Topic Modeling






